{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "field = Field(10,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Revised from 'alestainer'\n",
    "\"\"\"This is a gym-like environment for the game Wolfpack that allows multiple agents and message exchange between them\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "ACTION_MAP = {\n",
    "    'Stay': (0, 0),\n",
    "    'Up': (-1, 0),\n",
    "    'Left': (0, -1),\n",
    "    'Down': (1, 0),\n",
    "    'Right': (0, 1),\n",
    "    'U L': (-1, -1),\n",
    "    'U R': (-1, 1),\n",
    "    'D L': (1, -1),\n",
    "    'D R': (1, 1)\n",
    "}\n",
    "\n",
    "\n",
    "class Field(object):\n",
    "    def __init__(self, size, wall_density):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param size: Size of the field\n",
    "        :param wall_density: Probability for each cell that it will be wall there.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        field = np.zeros(shape=(size, size))  # Initialize field\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                if (field[i][j] != 2 and field[i][j] != 3):\n",
    "                    field[i][j] = np.random.binomial(1, wall_density)  # Create walls in the labyrinth\n",
    "        field.flags.writeable = False\n",
    "        self.field = field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Prey():\n",
    "    def __init__(self, field):\n",
    "        self.action_space = [0,1,2,3,4,5,6,7,8]\n",
    "        \n",
    "        \n",
    "        self.coordinates = tuple(np.random.randint(0, field.size, 2))\n",
    "        while (field.field[self.coordinates] == 1):\n",
    "            self.coordinates = tuple(np.random.randint(0, field.size, 2))\n",
    "    \n",
    "    def step_(self,action):\n",
    "        action = action\n",
    "        move = list(ACTION_MAP.values())[action]\n",
    "        coordinates = np.add(self.coordinates, move)\n",
    "        \n",
    "        while (coordinates[0] not in range(0, field.size)\n",
    "                or coordinates[1] not in range(0, field.size)\n",
    "                or field.field[tuple(coordinates)] == 1):\n",
    "            action = int(np.random.choice(action_space, 1))\n",
    "            move = list(ACTION_MAP.values())[action]\n",
    "            coordinates = np.add(self.coordinates, move)\n",
    "        self.coordinates = tuple(coordinates)\n",
    "        self.action = action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Hunt():\n",
    "    def __init__(self, field, prey):\n",
    "        self.action_space = [0,1,2,3,4,5,6,7,8]\n",
    "        \n",
    "        prey_coordinates = prey.coordinates\n",
    "        coordinates = tuple(np.random.randint(0, field.size, 2))\n",
    "        while (field.field[tuple(coordinates)] == 1 or (coordinates[0] == prey_coordinates[0] and coordinates[1] == prey_coordinates[1])):\n",
    "            coordinates = tuple(np.random.randint(0, field.size, 2))\n",
    "        self.coordinates = coordinates\n",
    "        \n",
    "    def step_(self,action):\n",
    "        action = action\n",
    "        move = list(ACTION_MAP.values())[action]\n",
    "        coordinates = np.add(self.coordinates, move)\n",
    "        \n",
    "        while (coordinates[0] not in range(0, field.size)\n",
    "                or coordinates[1] not in range(0, field.size)\n",
    "                or field.field[tuple(coordinates)] == 1):\n",
    "            action = int(np.random.choice(self.action_space, 1))\n",
    "            # action = 0 \n",
    "            move = list(ACTION_MAP.values())[action]\n",
    "            coordinates = np.add(self.coordinates, move)\n",
    "        self.coordinates = tuple(coordinates)\n",
    "        self.action = action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Prey at 0x14d06116b70>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prey(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix prey location\n",
    "class Game(object):\n",
    "    def __init__(self, num_of_hunters=2, reward_distance=2,\n",
    "                 field=Field(5, 0.1),prey=Prey(field)):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param num_of_hunters: Number of hunters playing\n",
    "        :param duration: Number of steps before interruption\n",
    "        :param reward_distance: distance from which hunter is getting reward\n",
    "        :param field: Field where the game is played\n",
    "        \"\"\"\n",
    "        self.field = field\n",
    "        self.done = False\n",
    "        self.reward_distance = reward_distance\n",
    "        self.size = field.size\n",
    "        self.num_of_hunters = num_of_hunters\n",
    "        self.prey = prey\n",
    "        \n",
    "        self.reset()  \n",
    "\n",
    "    def get_obs(self):\n",
    "        \n",
    "        observations = []\n",
    "        for i in range(len(self.hunters)):\n",
    "            original_field = self.field.field.copy()\n",
    "            hunter_postion = np.zeros(shape=(field.size,field.size))\n",
    "            hunter_postion[tuple(self.hunters[i].coordinates)] = 2\n",
    "            field_observation = np.zeros(shape=(field.size,field.size))\n",
    "            field_observation[tuple(self.prey.coordinates)] = 3\n",
    "            for j in range(len(self.hunters)):\n",
    "                if j != i:\n",
    "                    field_observation[tuple(self.hunters[j].coordinates)] = 2 \n",
    "            # assume the agent could see its own position and the other agent\n",
    "            field_final = np.stack((field_observation,hunter_postion,original_field),axis=2)\n",
    "            observations.append(field_final) # , message_observation\n",
    "        \n",
    "        return observations\n",
    "    \n",
    "    \n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Gives representation of the current game state\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        return sys.stdout.write(repr(self.state))\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the game with the same walls but with different prey and hunter coordinates\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        #prey = Prey(self.field)\n",
    "        hunters = []\n",
    "        for i in range(self.num_of_hunters):\n",
    "            hunters.append(Hunt(self.field, prey))\n",
    "        #self.prey = prey\n",
    "        self.hunters = hunters\n",
    "        \n",
    "        state = self.field.field.copy()\n",
    "        state[tuple(self.prey.coordinates)] = 3\n",
    "        for hunter in self.hunters:\n",
    "            state[tuple(hunter.coordinates)] = 2\n",
    "\n",
    "        self.state = state\n",
    "        self.done = False\n",
    "\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        :return: Vector of rewards\n",
    "        \"\"\"\n",
    "        rewards = np.zeros(len(self.hunters))\n",
    "        for i, hunter in enumerate(self.hunters):\n",
    "            if np.sqrt((hunter.coordinates[0] - self.prey.coordinates[0]) ** 2\n",
    "                               + (hunter.coordinates[1] - self.prey.coordinates[1]) ** 2) < self.reward_distance:\n",
    "                rewards[i] = 1\n",
    "\n",
    "        multiplier = sum(rewards)\n",
    "        return rewards * multiplier\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        One basic step in the environment\n",
    "        :param actions: Set of hunter's actions\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        # assume the prey do not move, save the trainning time\n",
    "        #action_space = [0,1,2,3,4,5,6,7,8]\n",
    "        #prey_action = int(np.random.choice(action_space, 1))\n",
    "        #self.prey.step_(prey_action)\n",
    "        \n",
    "        actions = actions\n",
    "        for i in range(self.num_of_hunters):\n",
    "            action = actions[i]\n",
    "            hunter = self.hunters[i]\n",
    "            hunter.step_(action)\n",
    "            \n",
    "        rewards = np.zeros(len(self.hunters))    \n",
    "        if any((x.coordinates[0] == self.prey.coordinates[0] and x.coordinates[1] == self.prey.coordinates[1]) \n",
    "               for x in self.hunters): \n",
    "            rewards = self.calculate_reward()\n",
    "            self.done = True\n",
    "\n",
    "\n",
    "        state = self.field.field.copy()\n",
    "        state[tuple(self.prey.coordinates)] = 3\n",
    "        for hunter in self.hunters:\n",
    "            state[tuple(hunter.coordinates)] = 2\n",
    "\n",
    "        self.state = state\n",
    "        \n",
    "        # collect the true action\n",
    "        for i in range(self.num_of_hunters):\n",
    "            hunter = self.hunters[i]\n",
    "            actions[i] = hunter.action\n",
    "        self.actions = actions\n",
    "        \n",
    "        return self.get_obs(), actions, rewards, self.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "field=Field(5, 0.1)\n",
    "prey=Prey(field)\n",
    "env = Game(num_of_hunters=2, reward_distance=2,field=field,prey=prey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 2],\n",
       "       [1, 3]])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi-agent dqn\n",
    "np.stack(([1,1],[1,2],[1,3]),axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D \n",
    "from keras.layers.core import Flatten, Lambda\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Merge\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import SpatialDropout2D\n",
    "from keras.layers import Dropout, Reshape\n",
    "from keras import backend as K\n",
    "\n",
    "import time\n",
    "\n",
    "input_height = 5\n",
    "input_width = 5\n",
    "input_channels = 3\n",
    "n_outputs = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # this applies 32 convolution filters of size 3x3 each.\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(5, 5, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    #model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))\n",
    "        \n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    #model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    #model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(9, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "X1_state = tf.placeholder(tf.float32,shape=[None,input_height,input_width,input_channels])\n",
    "X2_state = tf.placeholder(tf.float32,shape=[None,input_height,input_width,input_channels])\n",
    "\n",
    "critic_model_a1 = build_model()\n",
    "actor_model_a1 = build_model()\n",
    "critic_model_a2 = build_model()\n",
    "actor_model_a2 = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_q_a1 = actor_model_a1(X1_state)\n",
    "critic_q_a1 = critic_model_a1(X1_state)\n",
    "actor_q_a2 = actor_model_a2(X2_state)\n",
    "critic_q_a2 = critic_model_a2(X2_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'sequential_2/dense_4/Softmax:0' shape=(?, 9) dtype=float32>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_model_a1(X1_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "learning_rate = 0.001\n",
    "global_step = tf.Variable(0,trainable=False,name='global_step')\n",
    "X1_action = tf.placeholder(tf.int32,shape=[None])\n",
    "X2_action = tf.placeholder(tf.int32,shape=[None])\n",
    "\n",
    "q1_value = tf.reduce_sum(critic_q_a1 * tf.one_hot(X1_action,n_outputs),\n",
    "                       axis=1, keep_dims=True)\n",
    "q2_value = tf.reduce_sum(critic_q_a2 * tf.one_hot(X2_action,n_outputs),\n",
    "                       axis=1, keep_dims=True)\n",
    "\n",
    "y1=tf.placeholder(tf.float32,shape=[None,1])\n",
    "cost1 = tf.reduce_mean(tf.square(y1 - q1_value))\n",
    "\n",
    "y2=tf.placeholder(tf.float32,shape=[None,1])\n",
    "cost2 = tf.reduce_mean(tf.square(y2 - q1_value))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "optimizer2 = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "training_op = optimizer.minimize(cost1,global_step=global_step)\n",
    "training_op2 = optimizer.minimize(cost2,global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_memory_size = 10000\n",
    "replay_memory = deque([],maxlen=replay_memory_size)\n",
    "\n",
    "def sample_memories(batch_size):\n",
    "    indices = np.random.permutation(len(replay_memory))[:batch_size]\n",
    "    cols = [[],[],[],[],[]] # state, action, reward, next state, continue\n",
    "    for idx in indices:\n",
    "        memory = replay_memory[idx]\n",
    "        for col, value in zip(cols,memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return (cols[0],cols[1],cols[2].reshape(-1,1),cols[3],cols[4].reshape(-1,1))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps_min = 0.05\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 500000\n",
    "\n",
    "def epsilon_greedy(q_values,step):\n",
    "    epsilon = max(eps_min,eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs) # random action\n",
    "    else:\n",
    "        return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "n_steps = 2000000 # total number of training steps\n",
    "training_start = 1000 # start training after 1000 game iterations\n",
    "training_interval = 3 # run a training step every 3 game iterations\n",
    "save_steps = 100 # save the model every 50 training steps\n",
    "copy_steps = 50 # copy the critic to the actor every 25 training steps\n",
    "discount_rate = 0.95\n",
    "batch_size = 100\n",
    "iteration = 0 # game iteration\n",
    "done = True # env needs to be reset\n",
    "checkpoint_path = \"./re_dqn.ckpt\"\n",
    "loss = []\n",
    "rewards_re = []\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        saver.restore(sess,checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        iteration += 1\n",
    "        if env.done == 1:\n",
    "            env.reset()\n",
    "            \n",
    "        observation, actions, rewards, done = env.step([0,0])\n",
    "        state = observation\n",
    "        \n",
    "        #Actor evaluate waht to do\n",
    "        q1_values = actor_q_a1.eval(feed_dict={X1_state: [state[0]]})\n",
    "        action1 = epsilon_greedy(q1_values,step)\n",
    "        \n",
    "        q2_values = actor_q_a2.eval(feed_dict={X2_state: [state[1]]})\n",
    "        action2 = epsilon_greedy(q2_values,step)\n",
    "        \n",
    "        #Actor plays\n",
    "        observation, actions, rewards, done = env.step([action1,action2])\n",
    "        next_state = observation\n",
    "        \n",
    "        #Let's memory what just happened\n",
    "        replay_memory.append((state,actions,rewards,next_state,1.0 - done))\n",
    "        state = next_state\n",
    "        \n",
    "        if iteration < training_start or iteration % training_interval !=0:\n",
    "            continue\n",
    "            \n",
    "        # Critic learns\n",
    "        X_state_val, X_action_val, rewards, X_next_state_val, continues =(sample_memories(batch_size))\n",
    "        next_q1_values = actor_q_a1.eval(feed_dict = {X1_state: X_next_state_val[:,0,:,:]})\n",
    "        max_next_q1_values = np.max(next_q1_values,axis=1,keepdims=True)\n",
    "        next_q2_values = actor_q_a2.eval(feed_dict = {X2_state: X_next_state_val[:,1,:,:]})\n",
    "        max_next_q2_values = np.max(next_q2_values,axis=1,keepdims=True)\n",
    "        \n",
    "        \n",
    "        y1_val = rewards[0] + continues * discount_rate * max_next_q1_values\n",
    "        y2_val = rewards[1] + continues * discount_rate * max_next_q2_values\n",
    "        \n",
    "        \n",
    "        rewards_re.append(rewards)\n",
    "        loss.append(sess.run(cost1, feed_dict={X1_state:X_state_val[:,0,:,:],\n",
    "                                  X1_action:X_action_val[:,0] , y1: y1_val}))\n",
    "        training_op.run(feed_dict={X1_state:X_state_val[:,0,:,:],\n",
    "                                  X1_action:X_action_val[:,0] , y1: y1_val})\n",
    "        #training_op2.run(feed_dict={X2_state:X_state_val[:,1,:,:],\n",
    "                                  #X2_action: X_action_val[:,1], y2: y2_val})\n",
    "        \n",
    "        # Regularly copy critic to actor\n",
    "        if step % copy_steps == 0:\n",
    "            weight1 = critic_model_a1.get_weights()\n",
    "            actor_model_a1.set_weights(weight1)\n",
    "            actor_model_a2.set_weights(weight1)\n",
    "        # And save regularly\n",
    "        if step % save_steps == 0:\n",
    "            saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_re[1999996]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00026631972,\n",
       " 0.00011352457,\n",
       " 0.00023917008,\n",
       " 0.00041001767,\n",
       " 0.0001325461,\n",
       " 0.0005057129,\n",
       " 8.295872e-05,\n",
       " 0.00020509925,\n",
       " 0.0002441119,\n",
       " 0.00026955313,\n",
       " 0.00045035363,\n",
       " 0.00025890855,\n",
       " 0.00081632554,\n",
       " 0.00014287741,\n",
       " 0.0001264009,\n",
       " 0.00018416099,\n",
       " 0.00033605762,\n",
       " 0.0003725809,\n",
       " 0.00023257651,\n",
       " 0.00017871427,\n",
       " 0.00021325344,\n",
       " 0.00035591773,\n",
       " 0.00064733415,\n",
       " 0.00031373446,\n",
       " 0.00043554947,\n",
       " 0.0005582708,\n",
       " 0.00032029024,\n",
       " 8.471556e-05,\n",
       " 0.0005470502,\n",
       " 0.00017994583,\n",
       " 0.0011430888,\n",
       " 0.0008006306,\n",
       " 0.00063566933,\n",
       " 0.00046473276,\n",
       " 0.00040772613,\n",
       " 0.00053519517,\n",
       " 0.0004957472,\n",
       " 0.00013900489,\n",
       " 0.00020186784,\n",
       " 0.00035344486,\n",
       " 0.0001829499,\n",
       " 0.00016763735,\n",
       " 0.0005257513,\n",
       " 0.00011766648,\n",
       " 0.0006438201,\n",
       " 0.0005439863,\n",
       " 0.00036816485,\n",
       " 0.00086249097,\n",
       " 0.00031925223,\n",
       " 0.00027462206,\n",
       " 0.0006383363,\n",
       " 0.0005747929,\n",
       " 0.000244323,\n",
       " 0.00073792296,\n",
       " 0.00011131388,\n",
       " 0.00035925358,\n",
       " 0.0003413848,\n",
       " 0.00017986484,\n",
       " 0.00050967315,\n",
       " 0.00015065665,\n",
       " 0.0006740191,\n",
       " 0.0008163948,\n",
       " 0.00018080641,\n",
       " 0.00031725303,\n",
       " 0.0008838412,\n",
       " 0.0005428787,\n",
       " 0.00013132184,\n",
       " 0.000589192,\n",
       " 0.00021734856,\n",
       " 0.00023455959,\n",
       " 0.0006013285,\n",
       " 0.00047558875,\n",
       " 0.00016981119,\n",
       " 0.000517173,\n",
       " 0.00089164963,\n",
       " 0.00023434502,\n",
       " 0.00025232957,\n",
       " 0.00027141988,\n",
       " 0.00053217827,\n",
       " 0.0003928613,\n",
       " 7.849436e-05,\n",
       " 0.00017914429,\n",
       " 0.00021019459,\n",
       " 9.6629075e-05,\n",
       " 0.00028443563,\n",
       " 0.00024930525,\n",
       " 6.3200336e-05,\n",
       " 0.00041557717,\n",
       " 6.329034e-05,\n",
       " 0.0005846222,\n",
       " 0.0003570771,\n",
       " 0.00037327173,\n",
       " 0.00027089327,\n",
       " 0.00016651709,\n",
       " 0.00056508626,\n",
       " 0.00013573476,\n",
       " 0.0004936959,\n",
       " 0.0002945762,\n",
       " 0.0002129902,\n",
       " 0.00083604053]"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss[1999900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cumulative reward for each policy from the same matrix\n",
    "X_state1 = tf.placeholder(tf.float32,shape=[None,input_height,input_width,input_channels])\n",
    "X_state2 = tf.placeholder(tf.float32,shape=[None,input_height,input_width,input_channels])\n",
    "actor_v1 = actor_model_a1(X_state1)\n",
    "actor_v2 = actor_model_a1(X_state2)\n",
    "init = tf.global_variables_initializer()\n",
    "total_reward = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_action(q_values):\n",
    "    return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0., 0., 0., 0., 0.],\n",
      "       [3., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 2.],\n",
      "       [0., 2., 0., 0., 0.]])"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = []\n",
    "# cumulative reward for each policy\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    #iteration = 0\n",
    "    while True:\n",
    "        if len(total_reward) >= 100:\n",
    "            break\n",
    "        if env.done == 1:\n",
    "            #iteration += 1\n",
    "            env.reset()\n",
    "        #else:\n",
    "        reward = []\n",
    "        while env.done != 1:\n",
    "            observation, actions, rewards, done = env.step([0,0])\n",
    "            state = observation\n",
    "            #Actor evaluate what to do\n",
    "            q1_values = actor_v1.eval(feed_dict={X_state1: [state[0]]})\n",
    "            action1 = choose_action(q1_values)\n",
    "        \n",
    "            q2_values = actor_v2.eval(feed_dict={X_state2: [state[1]]})\n",
    "            action2 = choose_action(q2_values)\n",
    "        \n",
    "            #Actor plays\n",
    "            observation, actions, rewards, done = env.step([action1,action2])\n",
    "            reward = sum(rewards)\n",
    "            \n",
    "            next_state = observation\n",
    "            state = next_state\n",
    "            #print(env.done)\n",
    "        total_reward.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 1.0]"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADahJREFUeJzt3V2InOd5h/HrH0vBIU5rC223InK6ORApbqg/WIxbl5BG\ncXHtEOmgCAeSLsEgWtri0EJQctCSM+ckpC2lRdhut8RJI/JRCecLdeMQCqmTleMktuVUxsjERtJu\nnDi229Jg5+7BvgZV3tW8u7Oj2Xm4frDMOzPvau7HD1wezc6sUlVIkibf68Y9gCRpcxh0SWqEQZek\nRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRmy7lA+2c+fOmpmZuZQPKUkT78SJEz+uqqlB513S\noM/MzLC4uHgpH1KSJl6Sp/uc50suktQIgy5JjegV9CRXJvlckieSnEzyW0l2JDme5FR3edWoh5Uk\nra3vM/S/Br5aVb8OXAucBA4BC1W1B1jorkuSxmRg0JP8MvAO4F6Aqvp5VT0P7APmu9Pmgf2jGlKS\nNFifZ+hvBZaBf0zy3ST3JHkjMF1VZ7pzzgLTq31zkoNJFpMsLi8vb87UkqTX6BP0bcANwN9X1fXA\nf3HByyu18s8erfpPH1XV4aqararZqamBb6OUJG1Qn6A/AzxTVQ911z/HSuDPJdkF0F0ujWZESVIf\nA4NeVWeBHyV5W3fTXuBx4Bgw1902BxwdyYSSpF76flL0z4D7k7weeAr4ICv/MziS5E7gaeDAaEaU\npM0xc+hLY3vs03ffPvLH6BX0qnoEmF3lrr2bO44kaaP8pKgkNcKgS1IjDLokNcKgS1IjDLokNcKg\nS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1Ij\nDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjtvU5Kclp4EXgFeDlqppNsgP4LDAD\nnAYOVNVPRzOmJGmQ9TxD/92quq6qZrvrh4CFqtoDLHTXJUljMsxLLvuA+e54Htg//DiSpI3qG/QC\n/i3JiSQHu9umq+pMd3wWmF7tG5McTLKYZHF5eXnIcSVJa+n1GjrwO1X1bJJfAY4neeL8O6uqktRq\n31hVh4HDALOzs6ueI0kaXq9n6FX1bHe5BHwRuBE4l2QXQHe5NKohJUmDDQx6kjcmedOrx8DvAY8C\nx4C57rQ54OiohpQkDdbnJZdp4ItJXj3/01X11STfAY4kuRN4GjgwujElSYMMDHpVPQVcu8rtzwF7\nRzGUJGn9/KSoJDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXC\noEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtS\nIwy6JDXCoEtSI7b1PTHJZcAi8GxVvSfJDuCzwAxwGjhQVT8dxZAAM4e+NKo/eqDTd98+tseWpL7W\n8wz9LuDkedcPAQtVtQdY6K5LksakV9CT7AZuB+457+Z9wHx3PA/s39zRJEnr0fcZ+ieBDwO/OO+2\n6ao60x2fBaZX+8YkB5MsJllcXl7e+KSSpIsaGPQk7wGWqurEWudUVQG1xn2Hq2q2qmanpqY2Pqkk\n6aL6/FD0ZuC9SW4DLgd+KcmngHNJdlXVmSS7gKVRDipJuriBz9Cr6iNVtbuqZoA7gK9X1fuBY8Bc\nd9occHRkU0qSBhrmfeh3A7ckOQW8u7suSRqT3u9DB6iqbwDf6I6fA/Zu/kiSpI3wk6KS1AiDLkmN\nMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS\n1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNGBj0JJcn\n+XaS7yV5LMnHutt3JDme5FR3edXox5UkraXPM/T/Bd5VVdcC1wG3JrkJOAQsVNUeYKG7Lkkak4FB\nrxUvdVe3d18F7APmu9vngf0jmVCS1Euv19CTXJbkEWAJOF5VDwHTVXWmO+UsMD2iGSVJPfQKelW9\nUlXXAbuBG5O8/YL7i5Vn7a+R5GCSxSSLy8vLQw8sSVrdut7lUlXPAw8CtwLnkuwC6C6X1view1U1\nW1WzU1NTw84rSVpDn3e5TCW5sjt+A3AL8ARwDJjrTpsDjo5qSEnSYNt6nLMLmE9yGSv/AzhSVQ8k\n+RZwJMmdwNPAgRHOKUkaYGDQq+r7wPWr3P4csHcUQ0mS1s9PikpSIwy6JDXCoEtSIwy6JDXCoEtS\nIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6\nJDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDViYNCTXJ3kwSSPJ3ksyV3d7TuS\nHE9yqru8avTjSpLW0ucZ+svAX1TVNcBNwJ8kuQY4BCxU1R5gobsuSRqTgUGvqjNV9XB3/CJwEngz\nsA+Y706bB/aPakhJ0mDreg09yQxwPfAQMF1VZ7q7zgLTmzqZJGldegc9yRXA54EPVdUL599XVQXU\nGt93MMliksXl5eWhhpUkra1X0JNsZyXm91fVF7qbzyXZ1d2/C1ha7Xur6nBVzVbV7NTU1GbMLEla\nRZ93uQS4FzhZVZ84765jwFx3PAcc3fzxJEl9betxzs3AB4AfJHmku+2jwN3AkSR3Ak8DB0YzoiSp\nj4FBr6p/B7LG3Xs3dxxJ0kb5SVFJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJ\naoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRB\nl6RGGHRJaoRBl6RGGHRJaoRBl6RGDAx6kvuSLCV59LzbdiQ5nuRUd3nVaMeUJA3S5xn6PwG3XnDb\nIWChqvYAC911SdIYDQx6VX0T+MkFN+8D5rvjeWD/Js8lSVqnjb6GPl1VZ7rjs8D0Js0jSdqgoX8o\nWlUF1Fr3JzmYZDHJ4vLy8rAPJ0law0aDfi7JLoDucmmtE6vqcFXNVtXs1NTUBh9OkjTIRoN+DJjr\njueAo5szjiRpo/q8bfEzwLeAtyV5JsmdwN3ALUlOAe/urkuSxmjboBOq6n1r3LV3k2eRJA3BT4pK\nUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMM\nuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1\nYqigJ7k1yQ+TPJnk0GYNJUlavw0HPcllwN8Bvw9cA7wvyTWbNZgkaX2GeYZ+I/BkVT1VVT8H/gXY\ntzljSZLWa5igvxn40XnXn+lukySNwbZRP0CSg8DB7upLSX64wT9qJ/DjzZlqffLxTf8jx7aWEWhl\nLa2sA1zLlpSPD7WWX+tz0jBBfxa4+rzru7vb/p+qOgwcHuJxAEiyWFWzw/45W4Fr2XpaWQe4lq3q\nUqxlmJdcvgPsSfLWJK8H7gCObc5YkqT12vAz9Kp6OcmfAl8DLgPuq6rHNm0ySdK6DPUaelV9Gfjy\nJs0yyNAv22whrmXraWUd4Fq2qpGvJVU16seQJF0CfvRfkhqxpYKe5L4kS0keXeP+JPmb7lcNfD/J\nDZd6xr56rOWdSX6W5JHu6y8v9Yx9JLk6yYNJHk/yWJK7VjlnIval51omZV8uT/LtJN/r1vKxVc6Z\nlH3ps5aJ2BdY+RR9ku8meWCV+0a7J1W1Zb6AdwA3AI+ucf9twFeAADcBD4175iHW8k7ggXHP2WMd\nu4AbuuM3Af8JXDOJ+9JzLZOyLwGu6I63Aw8BN03ovvRZy0TsSzfrnwOfXm3eUe/JlnqGXlXfBH5y\nkVP2Af9cK/4DuDLJrksz3fr0WMtEqKozVfVwd/wicJLXfiJ4Ival51omQvff+qXu6vbu68IfiE3K\nvvRZy0RIshu4HbhnjVNGuidbKug9tPbrBn67+2vXV5L8xriHGSTJDHA9K8+gzjdx+3KRtcCE7Ev3\nV/tHgCXgeFVN7L70WAtMxr58Evgw8Is17h/pnkxa0FvyMPCWqvpN4G+Bfx3zPBeV5Arg88CHquqF\ncc8zjAFrmZh9qapXquo6Vj6lfWOSt497po3qsZYtvy9J3gMsVdWJcc0waUHv9esGJkFVvfDqXzNr\n5f3825PsHPNYq0qynZUA3l9VX1jllInZl0FrmaR9eVVVPQ88CNx6wV0Tsy+vWmstE7IvNwPvTXKa\nld8++64kn7rgnJHuyaQF/Rjwh91Pim8CflZVZ8Y91EYk+dUk6Y5vZGUvnhvvVK/VzXgvcLKqPrHG\naROxL33WMkH7MpXkyu74DcAtwBMXnDYp+zJwLZOwL1X1karaXVUzrPwqlK9X1fsvOG2kezLy37a4\nHkk+w8pPs3cmeQb4K1Z+QEJV/QMrn0q9DXgS+G/gg+OZdLAea/kD4I+TvAz8D3BHdT8G32JuBj4A\n/KB7jRPgo8BbYOL2pc9aJmVfdgHzWfmHZl4HHKmqB5L8EUzcvvRZy6Tsy2tcyj3xk6KS1IhJe8lF\nkrQGgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5Jjfg/CPS7tcNllpkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1621514e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(total_reward)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12947427 0.10257882 0.0778185  0.09702891 0.10729338 0.10723162\n",
      "  0.12512033 0.13942996 0.11402424]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(actor_v1.eval(feed_dict={X_state1: [state[0]]}))\n",
    "    #print(actor_model_a1([state[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    #observation, actions, rewards, done = env.step([0,0])\n",
    "    state = observation\n",
    "    #Actor evaluate what to do\n",
    "    q1_values = actor_v1.eval(feed_dict={X_state1: [state[0]]})\n",
    "    action1 = choose_action(q1_values)\n",
    "        \n",
    "    q2_values = actor_v2.eval(feed_dict={X_state2: [state[1]]})\n",
    "    action2 = choose_action(q2_values)\n",
    "        \n",
    "    #Actor plays\n",
    "    observation, actions, rewards, done = env.step([action1,action2])\n",
    "    reward += sum(rewards)\n",
    "            \n",
    "    next_state = observation\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0., 0., 2., 0., 0.],\n",
      "       [2., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0.]])"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12947427, 0.10257882, 0.0778185 , 0.09702891, 0.10729338,\n",
       "        0.10723162, 0.12512033, 0.13942996, 0.11402424]], dtype=float32)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14067793, 0.10261201, 0.08120742, 0.09758162, 0.1091956 ,\n",
       "        0.10892611, 0.11932769, 0.12620208, 0.11426958]], dtype=float32)"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
