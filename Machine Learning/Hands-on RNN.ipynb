{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Basic RNNs in Tensorflow, at each time, every neuron receives both \n",
    "# the input vector and the previous output vector\n",
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "X0 = tf.placeholder(tf.float32,[None,n_inputs])\n",
    "X1 = tf.placeholder(tf.float32,[None,n_inputs])\n",
    "\n",
    "Wx = tf.Variable(tf.random_normal(shape=[n_inputs,n_neurons],dtype=tf.float32))\n",
    "Wy = tf.Variable(tf.random_normal(shape=[n_neurons,n_neurons],dtype=tf.float32))\n",
    "b = tf.Variable(tf.zeros([1,n_neurons],dtype=tf.float32))\n",
    "\n",
    "Y0 = tf.tanh(tf.matmul(X0,Wx)+b)\n",
    "Y1 = tf.tanh(tf.matmul(Y0,Wy)+tf.matmul(X1,Wx)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.98983276  0.95793128 -0.43150836  0.61514908 -0.77770168]\n",
      " [-1.          0.99990988 -0.8087526   0.94929969 -1.        ]\n",
      " [-1.          0.99999982 -0.94523847  0.99433792 -1.        ]\n",
      " [-0.87197351 -0.99839008  0.99999183  0.99922323 -1.        ]]\n",
      "[[-1.          0.99999785 -0.96192461  0.2513243  -1.        ]\n",
      " [ 0.99727625 -0.77777004 -0.75489521 -0.98914921  0.63602072]\n",
      " [-0.9999994   0.99597675 -0.95873678 -0.80752027 -1.        ]\n",
      " [-0.98241305  0.96511012 -0.98859769 -0.63796723 -1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "init = tf.global_variables_initializer()\n",
    "# Mini-batch\n",
    "X0_batch = np.array([[0,1,2],[3,4,5],[6,7,8],[9,0,1]]) # t = 0\n",
    "X1_batch = np.array([[9,8,7],[0,0,0],[6,5,4],[3,2,1]]) # t = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    Y0_val, Y1_val = sess.run([Y0,Y1],feed_dict={X0: X0_batch,X1: X1_batch})\n",
    "    \n",
    "    print(Y0_val)\n",
    "    print(Y1_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Static Unrolling Through Time with sttatic_rnn().The following code creates exact the same model\n",
    "# like above, but is more simpler to write, especially when time steps are big.\n",
    "#X0 = tf.placeholder(tf.float32,[None,n_inputs])\n",
    "#X1 = tf.placeholder(tf.float32,[None,n_inputs])\n",
    "n_steps = 2\n",
    "X = tf.placeholder(tf.float32,[None,n_steps,n_inputs])\n",
    "X_seqs = tf.unstack(tf.transpose(X,perm=[1,0,2]))\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "# there are several cell exists, so made its a name to distinguish\n",
    "with tf.variable_scope('B1'):\n",
    "    output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell,X_seqs,dtype=tf.float32)\n",
    "\n",
    "outputs = tf.transpose(tf.stack(output_seqs),perm=[1,0,2])\n",
    "\n",
    "X_batch = np.array([\n",
    "    [[0,1,2],[9,8,7]],\n",
    "    [[3,4,5],[0,0,0]],\n",
    "    [[6,7,8],[6,5,4]],\n",
    "    [[9,0,1],[3,2,1]]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.71581197 -0.55254775  0.86902887  0.65668207  0.9197892 ]\n",
      "  [-1.         -0.99981415  1.         -0.12912703  0.99998623]]\n",
      "\n",
      " [[-0.99986368 -0.98185688  0.9999432   0.7779119   0.99957812]\n",
      "  [ 0.42606592 -0.25002331  0.08530296 -0.1822646  -0.24972039]]\n",
      "\n",
      " [[-0.99999982 -0.99941862  1.          0.85995275  0.99999774]\n",
      "  [-0.9999947  -0.99617112  0.99999642 -0.47372907  0.99778104]]\n",
      "\n",
      " [[-0.99998683 -0.99998415  0.99890566 -0.87006038  0.01382304]\n",
      "  [-0.99273354 -0.54457188  0.9352473   0.11324032  0.97421181]]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    outputs_val = outputs.eval(feed_dict={X: X_batch})\n",
    "    print(outputs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# there is a even better idea, dynamic_rnn() function\n",
    "n_steps=2\n",
    "X = tf.placeholder(tf.float32,[None,n_steps,n_inputs])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "with tf.variable_scope('B5'):\n",
    "    outputs, states = tf.nn.dynamic_rnn(basic_cell,X,dtype=tf.float32)\n",
    "    \n",
    "X_batch = np.array([\n",
    "    # t = 0   t = 1\n",
    "    [[0,1,2],[9,8,7]],\n",
    "    [[3,4,5],[0,0,0]],\n",
    "    [[6,7,8],[6,5,4]],\n",
    "    [[9,0,1],[3,2,1]]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.26147306  0.92727315  0.46621719  0.81288052  0.69008565]\n",
      "  [-0.97736037  1.          0.99983335  0.98398542  0.99794364]]\n",
      "\n",
      " [[-0.2551924   0.99997282  0.97632003  0.97420961  0.98658937]\n",
      "  [-0.2595697  -0.27985176  0.43371019  0.2821514  -0.58343893]]\n",
      "\n",
      " [[-0.65818655  1.          0.99921155  0.99669862  0.99950302]\n",
      "  [-0.87030524  0.99998784  0.9984799   0.82325637  0.95229805]]\n",
      "\n",
      " [[-0.98587334  0.99694037  0.9997443  -0.99392438 -0.58270109]\n",
      "  [-0.78614157  0.99458522  0.9759534  -0.32073846  0.61887091]]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    outputs_val = outputs.eval(feed_dict={X: X_batch})\n",
    "    print(outputs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# so far we have used only fixed-size input sequences(all exactly two steps long). \n",
    "# What if the input sequences have variable lengths.\n",
    "import numpy as np\n",
    "n_steps = 2\n",
    "X = tf.placeholder(tf.float32,[None,n_steps,n_inputs])\n",
    "X_seqs = tf.unstack(tf.transpose(X,perm=[1,0,2]))\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "seq_length = tf.placeholder(tf.int32,[None])\n",
    "# there are several cell exists, so made its a name to distinguish\n",
    "with tf.variable_scope('B2'):\n",
    "    output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell,X_seqs,dtype=tf.float32,\n",
    "                                                   sequence_length=seq_length)\n",
    "\n",
    "outputs = tf.transpose(tf.stack(output_seqs),perm=[1,0,2])\n",
    "\n",
    "X_batch = np.array([\n",
    "    [[0,1,2],[9,8,7]],\n",
    "    [[3,4,5],[0,0,0]],  # the [0,0,0] is to maintain the structure, but would not be used because of seq_length\n",
    "    [[6,7,8],[6,5,4]],\n",
    "    [[9,0,1],[3,2,1]]\n",
    "])\n",
    "\n",
    "seq_length_batch = np.array([2,1,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.7653302  -0.95297658 -0.60940617  0.14840634  0.82131851]\n",
      "  [-1.         -0.99998385 -0.99997109 -0.79619229 -0.99885327]]\n",
      "\n",
      " [[-0.99991739 -0.99978185 -0.9817003  -0.03475988  0.53253466]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-1.         -0.99999917 -0.99929756 -0.21562031  0.02649164]\n",
      "  [-0.99999815 -0.99436426 -0.99833828 -0.5840103  -0.99366611]]\n",
      "\n",
      " [[-0.99998748  0.98282105 -0.57022643 -0.98603582 -0.99996114]\n",
      "  [-0.99546218 -0.04556185  0.19253059 -0.21432495 -0.61927611]]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    outputs_val = outputs.eval(feed_dict={X: X_batch,seq_length:seq_length_batch})\n",
    "    print(outputs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we will treat each image as a sequence of 28 rows of 28 pixels.\n",
    "# we will use cells of 150 recurrent neurons, plus a fuly connected layer containing 10 neurons.\n",
    "# note that the fully connected layer is connected to the states tensor, contain only the final state of RNN.\n",
    "n_steps = 28\n",
    "n_inputs = 28\n",
    "n_neurons = 150\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,n_steps,n_inputs])\n",
    "y = tf.placeholder(tf.int32,[None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "with tf.variable_scope('B4'):\n",
    "    outputs, states = tf.nn.dynamic_rnn(basic_cell,X,dtype=tf.float32)\n",
    "\n",
    "# Logit is a function that maps probabilities ([0, 1]) to R ((-inf, inf))\n",
    "logits = tf.layers.dense(states,n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits,y,1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "0 Train accuracy 0.913333 Test accuracy: 0.9342\n",
      "1 Train accuracy 0.986667 Test accuracy: 0.9482\n",
      "2 Train accuracy 0.96 Test accuracy: 0.9535\n",
      "3 Train accuracy 0.98 Test accuracy: 0.9493\n",
      "4 Train accuracy 0.98 Test accuracy: 0.9642\n",
      "5 Train accuracy 0.973333 Test accuracy: 0.9706\n",
      "6 Train accuracy 0.966667 Test accuracy: 0.9662\n",
      "7 Train accuracy 0.973333 Test accuracy: 0.9666\n",
      "8 Train accuracy 0.986667 Test accuracy: 0.9723\n",
      "9 Train accuracy 0.98 Test accuracy: 0.9745\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "X_test = mnist.test.images.reshape((-1,n_steps,n_inputs))\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch = X_batch.reshape((-1,n_steps,n_inputs))\n",
    "            sess.run(training_op,feed_dict={X:X_batch,y:y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X:X_batch,y:y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X:X_test,y:y_test})\n",
    "        print(epoch,\"Train accuracy\",acc_train,\"Test accuracy:\",acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict time series\n",
    "# create RNN that contains 100 recurrent neurons and we will unroll it over 20 time steps\n",
    "n_steps = 12\n",
    "n_inputs = 1\n",
    "n_outputs =1\n",
    "n_neurons = 100\n",
    "X = tf.placeholder(tf.float32,[None,n_steps,n_inputs])\n",
    "y = tf.placeholder(tf.float32,[None])\n",
    "#cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons,activation=tf.nn.relu)\n",
    "# this make 100 size output linaer combined to 1.\n",
    "cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "        tf.contrib.rnn.BasicRNNCell(num_units=n_neurons,activation=tf.nn.relu),output_size = n_outputs)\n",
    "\n",
    "with tf.variable_scope('B6'):\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell,X,dtype=tf.float32)\n",
    "    \n",
    "learning_rate = 0.001\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(outputs-y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import math\n",
    "import numpy as np\n",
    "n_iterations = 100\n",
    "batch_size = 50\n",
    "\n",
    "T = pandas.read_csv('C:/COURSE/time series/Monthly Weather.csv',usecols=[2])\n",
    "dataset = T.values\n",
    "dataset = dataset.astype('float32')\n",
    "# normalize the dataset\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#dataset = scaler.fit_transform(dataset)\n",
    "#split to train and test sets\n",
    "train_size = 240\n",
    "test_size = 120\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "# let X denote the temperature of time t and Y denote the time (t+1)\n",
    "# convert an array of values into a matrix\n",
    "def create_dataset(dataset, look_back):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "#reshape into X = t, Y = t+12\n",
    "look_back = 12\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "#reshape input tobe [samples,timesteps,features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0],trainX.shape[1],1))\n",
    "testX = np.reshape(testX, (testX.shape[0], testX.shape[1],1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tMSE: 3132.54\n",
      "10 \tMSE: 1615.17\n",
      "20 \tMSE: 796.671\n",
      "30 \tMSE: 737.629\n",
      "40 \tMSE: 714.667\n",
      "50 \tMSE: 685.993\n",
      "60 \tMSE: 658.467\n",
      "70 \tMSE: 637.905\n",
      "80 \tMSE: 617.746\n",
      "90 \tMSE: 603.398\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for iteration in range(n_iterations):\n",
    "        X_batch, y_batch = trainX,trainY\n",
    "        sess.run(training_op, feed_dict={X:X_batch,y:y_batch})\n",
    "        if iteration % 10 == 0:\n",
    "            mse = loss.eval(feed_dict={X:X_batch,y:y_batch})\n",
    "            print(iteration,\"\\tMSE:\",mse)\n",
    "            \n",
    "    X_new = testX\n",
    "    y_pred = sess.run(outputs,feed_dict={X:X_new})\n",
    "    #print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rather than using OutputProjectionWrapper, there is a more effecient way to do so.\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons,activation=tf.nn.relu)\n",
    "rnn_outputs,states = tf.nn.dynamic_rnn(cell,X,dtype=tf.float32)\n",
    "\n",
    "stacked_rnn_outputs = tf.reshape(rnn_outputs,[-1,n_neurons])\n",
    "stacked_outputs = tf.layers.dense(stacked_rnn_outputs,n_outputs)\n",
    "outputs = tf.reshape(stacked_outputs,[-1,n_steps,n_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deep RNN\n",
    "n_neurons = 100\n",
    "n_layers = 3\n",
    "\n",
    "layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons,activation=tf.nn.relu)\n",
    "             for layer in range(n_layers)]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell,X,dtype=tf.float32)\n",
    "\n",
    "# dropout algorithm\n",
    "# at every training step, every neuron has a probability p of being temporarily ignored.It typically to be set to 50%.\n",
    "\n",
    "keep_prob = 0.5\n",
    "\n",
    "cells = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "             for layer in range(n_layers)]\n",
    "cells_drop = [tf.contrib.rnn.DropoutWrapper(cell,input_keep_prob=keep_prob)\n",
    "             for cell in cells]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(cells_drop)\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell,X,dtype=tf.float32)\n",
    "\n",
    "# LSTM\n",
    "lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
